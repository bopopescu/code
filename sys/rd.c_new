/*

    @todo:
    1. converge fedora and ubu vers
    2. implement fops (pg 466)
    3. ioctl HDIO_GETGEO
    4. format, app, ...
    5. spin lock for queue
    6. queue depth? tags?
    7. blk_cleanup_queue on exit
    8. make RAID device using stacking - see pg 491
            return non-zero from make_request
            but first modify bi_bdev to point 
            to another device.. Possibly us
            bio_split.

*/

#include <linux/bio.h>
#include <linux/bitops.h>
#include <linux/blkdev.h>
#include <linux/delay.h>
#include <linux/errno.h>
#include <linux/fs.h>
#include <linux/genhd.h>
#include <linux/idr.h>
#include <linux/init.h>
#include <linux/interrupt.h>
#include <linux/io.h>
#include <linux/kthread.h>
#include <linux/kernel.h>
#include <linux/mm.h>
#include <linux/module.h>
#include <linux/moduleparam.h>
#include <linux/ptrace.h>
#include <linux/sched.h>
#include <linux/slab.h>
#include <linux/types.h>
#include <asm-generic/io-64-nonatomic-lo-hi.h>

#define SPACE_SIZE 10*1024*1024
#define SIZE_SECTOR 512
struct rd_dev {
    int instance;
    u32 max_hw_sectors;
    struct request_queue *queue;
    struct gendisk *disk;
    unsigned ns_id;
    int lba_shift;
    int size;
    char *data;
};

static int rd_major;
module_param(rd_major, int, 0);
struct rd_dev *g_dev = NULL;

void write(struct rd_dev *dev, char *buffer, unsigned long sector, int size)
{
    unsigned long offset = sector * SIZE_SECTOR;
    memcpy(&dev->data[offset], buffer, size);
}

void read(struct rd_dev *dev, char *buffer, unsigned long sector, int size)
{
    unsigned long offset = sector * SIZE_SECTOR;
    memcpy(buffer, &dev->data[offset], size);
}

static void transfer(struct rd_dev *dev, unsigned long sector,
        unsigned long nsect, char *buffer, int write)
{
    unsigned long offset = sector*SIZE_SECTOR;
    unsigned long nbytes = nsect*SIZE_SECTOR;

    if ((offset + nbytes) > dev->size) {
        printk (KERN_NOTICE "Beyond-end write (%ld %ld)\n", offset, nbytes);
        return;
    }
    if (write)
        memcpy(dev->data + offset, buffer, nbytes);
    else
        memcpy(buffer, dev->data + offset, nbytes);
}

/* NVMe scatterlists require no holes in the virtual address */
#define BIOVEC_NOT_VIRT_MERGEABLE(vec1, vec2)	((vec2)->bv_offset || \
			(((vec1)->bv_offset + (vec1)->bv_len) % PAGE_SIZE))
static int rd_xfr_bio(struct rd_dev *dev, struct bio *bio)
{
    struct bio_vec bvec, bvprv = {0};
    struct bvec_iter iter_segno;
    //int segno;
    char *base_addr = NULL;
    int first = 1, length = 0, nsegs = 0, split_len = bio->bi_iter.bi_size;
    int psegs = bio_phys_segments(dev->queue, bio);

    if ((bio->bi_rw & REQ_FLUSH) && psegs)
        printk("flusher...\n");

    printk("psegs %x\n", psegs);
    printk("bi_iter.bi_size %x\n", bio->bi_iter.bi_size);
    printk("bi_rw           %lx\n", bio->bi_rw);
    if (bio->bi_rw & REQ_DISCARD)
        printk("discard...\n");
    if ((bio->bi_rw & REQ_FLUSH) && !psegs)
        printk("no data flusher...\n");
    if (bio->bi_rw & REQ_FUA)
        printk("requesting FUA...\n");
    if (bio->bi_rw & (REQ_FAILFAST_DEV | REQ_RAHEAD))
        printk("lazy reader...\n");
    if (bio->bi_rw & REQ_RAHEAD)
        printk("prefetcher...\n");
    printk("sector %lx\n", bio->bi_iter.bi_sector);
    base_addr = bio_data(bio);
    printk("base address %p\n", base_addr);
    bio_for_each_segment(bvec, bio, iter_segno) {
        if (!first && BIOVEC_PHYS_MERGEABLE(&bvprv, &bvec)) {
            length += bvec.bv_len;
        } else {
            if (!first && BIOVEC_NOT_VIRT_MERGEABLE(&bvprv, &bvec)) {
                printk("need to split ...\n");
                break;
            }
      		printk("bvec.bv_page.mapping %p, bvec.bv_len %x bvec.bv_offset %x\n", 
                  			bvec.bv_page->mapping, bvec.bv_len, bvec.bv_offset);

            nsegs++;
        }
        if (split_len - length < bvec.bv_len) {
            printk("need to split again ...\n");
            break;
        }
        length += bvec.bv_len;
        bvprv = bvec;
        first = 0;
    }
    printk("length %d, split len %d, nsegs %d\n", length, split_len, nsegs);
    if (bio_data_dir(bio)) {
        printk("writer...\n");
        write(dev, base_addr, bio->bi_iter.bi_sector, length);
        //write(dev, base_addr, bio->bi_sector, length);
    } else {
        printk("reader...\n");
        read(dev, base_addr, bio->bi_iter.bi_sector, length);
        //read(dev, base_addr, bio->bi_sector, length);
    }
    return 0;
}

static void rd_make_request(struct request_queue *q, struct bio *bio)
{
    struct rd_dev *dev = q->queuedata;
    int rc;
    rc = rd_xfr_bio(dev, bio);
    bio_endio(bio, rc);
    return;
}

static int rd_ioctl(struct block_device *bdev, fmode_t mode, unsigned int cmd,
                    unsigned long arg)
{
    return 0;
}

#ifdef CONFIG_COMPAT
static int rd_compat_ioctl(struct block_device *bdev, fmode_t mode,
                    unsigned int cmd, unsigned long arg)
{
	return rd_ioctl(bdev, mode, cmd, arg);
}
#else
#define rd_compat_ioctl	NULL
#endif

static int rd_open(struct block_device *bdev, fmode_t mode)
{
    return 0;
}

static void rd_release(struct gendisk *disk, fmode_t mode)
{
//    return 0;
}

static const struct block_device_operations rd_fops = {
    .owner		= THIS_MODULE,
    .ioctl		= rd_ioctl,
    .compat_ioctl	= rd_compat_ioctl,
    .open		= rd_open,
    .release	= rd_release,
};

static int rd_alloc_ns(struct rd_dev *dev, unsigned nsid)
{
    struct gendisk *disk;
    dev->queue = blk_alloc_queue(GFP_KERNEL);
    if (!dev->queue)
        goto out_free_ns;
    dev->queue->queue_flags = QUEUE_FLAG_DEFAULT;
    queue_flag_set_unlocked(QUEUE_FLAG_NOMERGES, dev->queue);
    queue_flag_set_unlocked(QUEUE_FLAG_NONROT, dev->queue);
    blk_queue_make_request(dev->queue, rd_make_request);
    dev->queue->queuedata = dev;
    disk = alloc_disk(0);
    if (!disk)
        goto out_free_queue;
    dev->ns_id = nsid;
    dev->disk = disk;
    dev->lba_shift = 9; //id->lbaf[lbaf].ds;
    blk_queue_logical_block_size(dev->queue, 1 << dev->lba_shift);
    if (dev->max_hw_sectors)
        blk_queue_max_hw_sectors(dev->queue, dev->max_hw_sectors);
    disk->major = rd_major;
    disk->first_minor = 0;
    disk->fops = &rd_fops;
    disk->private_data = dev;
    disk->queue = dev->queue;
    disk->flags = GENHD_FL_EXT_DEVT;
    sprintf(disk->disk_name, "rd%dn%d", 0, nsid);
    set_capacity(disk, SPACE_SIZE / SIZE_SECTOR); // 10Meg
    return 0;
out_free_queue:
    blk_cleanup_queue(dev->queue);
out_free_ns:
    return -1;
}

static void rd_dev_remove(struct rd_dev *dev)
{
    if (dev->disk->flags & GENHD_FL_UP) {
        del_gendisk(dev->disk);
    }
    if (!blk_queue_dying(dev->queue)) {
        blk_cleanup_queue(dev->queue);
    }
    put_disk(dev->disk);
}

static int __init rd_init(void)
{
    int result;
    struct rd_dev *dev;
    result = register_blkdev(rd_major, "rd");
    if (result < 0)
        goto error_exit;
    else if (result > 0)
        rd_major = result;
    dev = kzalloc(sizeof(*dev), GFP_KERNEL);
    if (!dev)
        return -ENOMEM;
    dev->size = SPACE_SIZE;
    dev->data = vmalloc(SPACE_SIZE);
    if (!dev->data) {
        kfree(dev);
        return -ENOMEM;
    }
    memset(dev->data, 0, SPACE_SIZE);
    g_dev = dev;
    dev->instance = 0;
    result = rd_alloc_ns(dev, 0);
    if (result) {
        goto error_exit;
    }
    add_disk(dev->disk);
    result = 0;
    printk("loading rd..\n");
error_exit:
    return result;
}

static void __exit rd_exit(void)
{
    struct rd_dev *dev = g_dev;
    rd_dev_remove(dev);
    unregister_blkdev(rd_major, "rd");
    vfree(dev->data);
    kfree(dev);
    printk("unloading rd..\n");
}

MODULE_LICENSE("GPL");
MODULE_VERSION("0.8");
module_init(rd_init);
module_exit(rd_exit);
